{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Quantization Benchmark\n",
    "## Comparing 7 quantization methods on Qwen2-0.5B\n",
    "\n",
    "Methods covered:\n",
    "| Method | Type | Bits | Key Idea |\n",
    "|---|---|---|---|\n",
    "| **FP16 Baseline** | — | 16 | No quantization |\n",
    "| **RTN INT8** | PTQ | 8 | Round-to-nearest, per-channel scale |\n",
    "| **RTN INT4** | PTQ | 4 | Same, more aggressive |\n",
    "| **SmoothQuant** | PTQ | 8 | Migrate outliers from activations to weights |\n",
    "| **GPTQ** | PTQ | 4 | Hessian-based error compensation, column-wise |\n",
    "| **AWQ** | PTQ | 4 | Activation-aware weight channel scaling |\n",
    "| **Palletization 4-bit** | Codebook | 4 | K-means LUT, Apple CoreML style |\n",
    "| **QLoRA NF4** | PTQ | ~4.1 | Normal Float quantile grid + double quant |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS (Apple Silicon)' if torch.backends.mps.is_available() else 'CPU'}\")\n",
    "\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "MODEL_ID = 'Qwen/Qwen2-0.5B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Intuition: What each method does to the weight distribution"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from quant_methods.palletization import kmeans_lut, uniform_lut, NF4_LEVELS\n",
    "from quant_methods.qlora_nf4 import NF4_LEVELS\n",
    "\n",
    "# Simulate a realistic weight distribution (like an attention projection)\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(256) * 0.02  # typical LLM weight scale\n",
    "\n",
    "bits = 4\n",
    "n_entries = 2 ** bits  # 16\n",
    "qmax = n_entries // 2 - 1  # 7 for INT4\n",
    "\n",
    "# ── Method 1: RTN INT4 (uniform grid) ────────────────────────────────────────\n",
    "scale_rtn = W.abs().max() / qmax\n",
    "W_rtn = (W / scale_rtn).round().clamp(-qmax-1, qmax) * scale_rtn\n",
    "rtn_levels = torch.arange(-qmax-1, qmax+1).float() * scale_rtn\n",
    "\n",
    "# ── Method 2: K-means Palletization ──────────────────────────────────────────\n",
    "km_lut, km_idx = kmeans_lut(W, n_entries)\n",
    "W_pall = km_lut[km_idx]\n",
    "\n",
    "# ── Method 3: NF4 ────────────────────────────────────────────────────────────\n",
    "absmax = W.abs().max()\n",
    "W_norm = W / absmax\n",
    "dists = (W_norm.unsqueeze(1) - NF4_LEVELS.unsqueeze(0)).abs()\n",
    "nf4_idx = dists.argmin(dim=1)\n",
    "W_nf4 = NF4_LEVELS[nf4_idx] * absmax\n",
    "nf4_levels_scaled = NF4_LEVELS * absmax\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Quantization Grid vs Weight Distribution\\n(same 4-bit budget, different level placement)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "methods = [\n",
    "    ('RTN INT4\\n(uniform grid)', W_rtn, rtn_levels, '#e74c3c'),\n",
    "    ('Palletization 4-bit\\n(k-means LUT)', W_pall, km_lut, '#2ecc71'),\n",
    "    ('NF4\\n(normal quantile grid)', W_nf4, nf4_levels_scaled, '#3498db'),\n",
    "]\n",
    "\n",
    "for ax, (title, W_q, levels, color) in zip(axes, methods):\n",
    "    # Original distribution\n",
    "    ax.hist(W.numpy(), bins=40, alpha=0.4, color='gray', label='Original')\n",
    "    ax.hist(W_q.numpy(), bins=40, alpha=0.6, color=color, label='Quantized')\n",
    "    \n",
    "    # Quantization levels as vertical lines\n",
    "    for lv in levels:\n",
    "        ax.axvline(lv.item(), color=color, alpha=0.3, linewidth=0.8)\n",
    "    \n",
    "    mse = (W_q - W).pow(2).mean().item()\n",
    "    ax.set_title(f'{title}\\nMSE = {mse:.2e}', fontsize=11)\n",
    "    ax.set_xlabel('Weight value')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/weight_distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Key insight: k-means and NF4 cluster their levels where weights are dense')\n",
    "print('RTN wastes levels on sparse regions near ±max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Palletization deep-dive: K-means vs Uniform LUT vs RTN"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_methods.palletization import compare_lut_methods\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print('Loading Qwen2-0.5B to compare LUT methods on real weights...')\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "results_lut = {}\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'weight') and module.weight is not None and 'Linear' in type(module).__name__:\n",
    "        if module.weight.numel() > 1000:  # skip tiny layers\n",
    "            results_lut[name] = compare_lut_methods(module.weight.data, bits=4)\n",
    "        if len(results_lut) >= 20:\n",
    "            break\n",
    "\n",
    "# Summary statistics\n",
    "km_mses   = [r['kmeans_palletization_mse'] for r in results_lut.values()]\n",
    "uni_mses  = [r['uniform_palletization_mse'] for r in results_lut.values()]\n",
    "rtn_mses  = [r['rtn_int4_mse'] for r in results_lut.values()]\n",
    "improvements = [r['kmeans_vs_rtn_improvement'] for r in results_lut.values()]\n",
    "\n",
    "print(f'\\nAverage MSE across {len(results_lut)} layers:')\n",
    "print(f'  RTN INT4:              {np.mean(rtn_mses):.2e}')\n",
    "print(f'  Uniform Palletization: {np.mean(uni_mses):.2e}')\n",
    "print(f'  K-means Palletization: {np.mean(km_mses):.2e}')\n",
    "print(f'\\nK-means improvement over RTN: {np.mean(improvements):.2f}x lower MSE')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "x = np.arange(len(results_lut))\n",
    "w = 0.28\n",
    "ax1.bar(x - w, rtn_mses,  w, label='RTN INT4',              color='#e74c3c', alpha=0.85)\n",
    "ax1.bar(x,     uni_mses,  w, label='Uniform Palletization', color='#f39c12', alpha=0.85)\n",
    "ax1.bar(x + w, km_mses,   w, label='K-means Palletization', color='#2ecc71', alpha=0.85)\n",
    "ax1.set_xlabel('Layer index')\n",
    "ax1.set_ylabel('Quantization MSE')\n",
    "ax1.set_title('Quantization MSE per Layer\\n(lower = better, same 4-bit budget)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_xticks(x)\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "ax1.set_xticklabels([k.split('.')[-1] for k in results_lut.keys()], rotation=45)\n",
    "\n",
    "ax2.hist(improvements, bins=15, color='#2ecc71', edgecolor='white', alpha=0.85)\n",
    "ax2.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Break-even (1x)')\n",
    "ax2.axvline(np.mean(improvements), color='#27ae60', linestyle='--',\n",
    "            linewidth=2, label=f'Mean: {np.mean(improvements):.2f}x')\n",
    "ax2.set_xlabel('MSE improvement ratio (k-means / RTN MSE, higher = k-means wins more)')\n",
    "ax2.set_ylabel('Layer count')\n",
    "ax2.set_title('Distribution of K-means Palletization\\nImprovement over RTN INT4')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/palletization_vs_rtn.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Run Full Benchmark (takes 20-40 min depending on hardware)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark — you can subset methods for quick iteration:\n",
    "# e.g. methods_to_run = ['fp16_baseline', 'rtn_int8', 'rtn_int4', 'palletization_4bit']\n",
    "\n",
    "from benchmark import run_benchmark, METHODS\n",
    "\n",
    "methods_to_run = list(METHODS.keys())  # all methods\n",
    "results = run_benchmark(methods_to_run, Path('results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load pre-computed results\n",
    "# with open('results/results.json') as f:\n",
    "#     results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Results Visualization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load and sort results ─────────────────────────────────────────────────────\n",
    "with open('results/results.json') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "methods  = [r['label'] for r in results.values()]\n",
    "ppls     = [r['perplexity'] for r in results.values()]\n",
    "mems     = [r['memory_mb'] for r in results.values()]\n",
    "lats     = [r['latency_ms_token'] for r in results.values()]\n",
    "comprs   = [r['compression_ratio'] for r in results.values()]\n",
    "bits_arr = [r['bits'] for r in results.values()]\n",
    "\n",
    "# Color by method type\n",
    "type_colors = {\n",
    "    'FP16 Baseline':       '#95a5a6',\n",
    "    'RTN INT8':            '#e74c3c',\n",
    "    'RTN INT4':            '#c0392b',\n",
    "    'GPTQ INT4':           '#8e44ad',\n",
    "    'AWQ INT4':            '#2980b9',\n",
    "    'SmoothQuant INT8':    '#e67e22',\n",
    "    'Palletization 4-bit': '#27ae60',\n",
    "    'QLoRA NF4':           '#16a085',\n",
    "}\n",
    "colors = [type_colors.get(m, '#7f8c8d') for m in methods]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.suptitle('LLM Quantization Benchmark: Qwen2-0.5B\\n'\n",
    "             'All methods at comparable bit-width | Lower PPL = better | Lower memory = better',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2, hspace=0.4, wspace=0.35)\n",
    "\n",
    "# ── Plot 1: Perplexity (lower = better) ──────────────────────────────────────\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "bars = ax1.barh(methods, ppls, color=colors, edgecolor='white', height=0.6)\n",
    "ax1.set_xlabel('Perplexity (lower = better)')\n",
    "ax1.set_title('Perplexity on WikiText-2')\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{ppl:.2f}', va='center', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# ── Plot 2: Memory footprint ──────────────────────────────────────────────────\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars2 = ax2.barh(methods, mems, color=colors, edgecolor='white', height=0.6)\n",
    "ax2.set_xlabel('Memory Footprint (MB) — lower = better')\n",
    "ax2.set_title('Model Memory Usage')\n",
    "for bar, m in zip(bars2, mems):\n",
    "    ax2.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n",
    "             f'{m:.0f}MB', va='center', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# ── Plot 3: PPL vs Memory scatter (Pareto frontier) ──────────────────────────\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "for m, ppl, mem, color in zip(methods, ppls, mems, colors):\n",
    "    ax3.scatter(mem, ppl, color=color, s=120, zorder=5, edgecolors='white', linewidth=1.5)\n",
    "    ax3.annotate(m, (mem, ppl), textcoords='offset points', xytext=(6, 3),\n",
    "                 fontsize=8, color=color)\n",
    "ax3.set_xlabel('Memory (MB)')\n",
    "ax3.set_ylabel('Perplexity')\n",
    "ax3.set_title('Quality vs Memory Trade-off\\n(bottom-left = Pareto optimal)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ── Plot 4: Latency comparison ────────────────────────────────────────────────\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "bars4 = ax4.barh(methods, lats, color=colors, edgecolor='white', height=0.6)\n",
    "ax4.set_xlabel('Latency (ms / token) — lower = better')\n",
    "ax4.set_title('Decode Latency (autoregressive)')\n",
    "for bar, lat in zip(bars4, lats):\n",
    "    ax4.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height()/2,\n",
    "             f'{lat:.1f}ms', va='center', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Legend\n",
    "patches = [mpatches.Patch(color=c, label=m) for m, c in type_colors.items()]\n",
    "fig.legend(handles=patches, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05),\n",
    "           fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.savefig('results/benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Summary Table"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': r['label'],\n",
    "        'Bits': r['bits'],\n",
    "        'PPL ↓': r['perplexity'],\n",
    "        'Δ PPL vs FP16': round(r['perplexity'] - results['fp16_baseline']['perplexity'], 3),\n",
    "        'Memory (MB) ↓': r['memory_mb'],\n",
    "        'Compress ↑': f\"{r['compression_ratio']:.2f}x\",\n",
    "        'Latency (ms/tok) ↓': r['latency_ms_token'],\n",
    "    }\n",
    "    for r in results.values()\n",
    "]).sort_values('PPL ↓')\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print('\\n── Key Takeaways ──────────────────────────────────────────────────')\n",
    "print('Palletization 4-bit:')\n",
    "print('  • Non-uniform LUT adapts to weight distribution')\n",
    "print('  • Comparable PPL to GPTQ/AWQ without Hessian computation')\n",
    "print('  • Apple ANE has native gather-LUT support → real speedup on-device')\n",
    "print('  • CoreML: use coremltools.optimize.torch.palettization.DKMPalettizer')\n",
    "print()\n",
    "print('GPTQ/AWQ:')\n",
    "print('  • Best PTQ perplexity at INT4 — use when accuracy is paramount')\n",
    "print('  • GPTQ: better quality, slower to quantize (Hessian per layer)')\n",
    "print('  • AWQ: slightly worse PPL, much faster to quantize (activation stats only)')\n",
    "print()\n",
    "print('NF4:')\n",
    "print('  • Best effective bits/weight with double quantization (~4.127 bpw)')\n",
    "print('  • Designed for QLoRA fine-tuning, not pure inference')\n",
    "print()\n",
    "print('SmoothQuant:')\n",
    "print('  • Enables W8A8 quantization (weight + activation INT8)')\n",
    "print('  • Real benefit requires INT8 GEMM kernel (NVIDIA/ANE hardware)')\n",
    "print('  • PPL sits between RTN INT8 and GPTQ INT4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CoreML Palletization (macOS only)\n",
    "\n",
    "This cell converts the Qwen2-0.5B model to CoreML with real 4-bit palletization\n",
    "using Apple's DKM (Differentiable K-means) algorithm, which is the production\n",
    "version of what we implemented above.\n",
    "\n",
    "```python\n",
    "# Run this on macOS with coremltools >= 7.0\n",
    "# pip install coremltools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CoreML / Apple-native palletization ──────────────────────────────────────\n",
    "# Requires: macOS, coremltools >= 7.0\n",
    "# pip install coremltools\n",
    "\n",
    "import sys\n",
    "\n",
    "def run_coreml_palletization():\n",
    "    try:\n",
    "        import coremltools as ct\n",
    "        from coremltools.optimize.torch.palettization import (\n",
    "            DKMPalettizer,\n",
    "            DKMPalettizerConfig,\n",
    "            PostTrainingPalettizer,\n",
    "            PostTrainingPalettizerConfig,\n",
    "        )\n",
    "    except ImportError:\n",
    "        print('coremltools not available. Install with: pip install coremltools')\n",
    "        print('Requires macOS.')\n",
    "        return\n",
    "\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print('Loading model...')\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "    # ── Option A: Post-training palletization (no fine-tuning, fast) ──────────\n",
    "    print('\\nApplying Post-Training Palletization (4-bit, k-means, no fine-tuning)...')\n",
    "    pt_config = PostTrainingPalettizerConfig.from_dict({\n",
    "        'global_config': {\n",
    "            'n_bits': 4,\n",
    "            'granularity': 'per_grouped_channel',\n",
    "            'group_size': 128,\n",
    "            'enable_per_channel_scale': True,  # finer scale, better quality\n",
    "        }\n",
    "    })\n",
    "    pt_palettizer = PostTrainingPalettizer(model, pt_config)\n",
    "    palettized_model = pt_palettizer.compress()\n",
    "\n",
    "    # ── Option B: DKM Palettizer (with fine-tuning, best quality) ─────────────\n",
    "    # print('\\nApplying DKM Palettization (fine-tuning aware)...')\n",
    "    # dkm_config = DKMPalettizerConfig.from_dict({\n",
    "    #     'global_config': {\n",
    "    #         'n_bits': 4,\n",
    "    #         'cluster_dim': 1,\n",
    "    #         'enable_per_channel_scale': True,\n",
    "    #     }\n",
    "    # })\n",
    "    # palettizer = DKMPalettizer(model, dkm_config)\n",
    "    # palettizer.prepare()\n",
    "    # # ... insert fine-tuning loop here ...\n",
    "    # palettized_model = palettizer.finalize()\n",
    "\n",
    "    # ── Convert to CoreML ──────────────────────────────────────────────────────\n",
    "    print('\\nConverting to CoreML .mlpackage...')\n",
    "    example_input = tokenizer('Hello world', return_tensors='pt')\n",
    "    traced = torch.jit.trace(palettized_model, (example_input['input_ids'],))\n",
    "\n",
    "    mlmodel = ct.convert(\n",
    "        traced,\n",
    "        inputs=[ct.TensorType(name='input_ids', shape=example_input['input_ids'].shape)],\n",
    "        outputs=[ct.TensorType(name='logits')],\n",
    "        minimum_deployment_target=ct.target.iOS17,  # ANE support\n",
    "        compute_units=ct.ComputeUnit.ALL,            # CPU + GPU + ANE\n",
    "    )\n",
    "\n",
    "    mlmodel.save('results/qwen2_palletized_4bit.mlpackage')\n",
    "    print('Saved to results/qwen2_palletized_4bit.mlpackage')\n",
    "    print('\\nModel spec summary:')\n",
    "    print(mlmodel.get_spec().description)\n",
    "\n",
    "run_coreml_palletization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
